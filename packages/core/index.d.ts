/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface InferenceToken {
  token: string
  completed: boolean
}
export interface LoadModelResult {
  error: boolean
  message?: string
}
export interface LLamaConfig {
  path: string
  numCtxTokens?: number
}
export interface LLamaArguments {
  nThreads?: number
  nBatch?: bigint
  topK?: bigint
  topP?: number
  repeatPenalty?: number
  temp?: number
  seed?: bigint
  numPredict?: bigint
  repeatLastN?: bigint
  prompt: string
}
export class LLama {
  static enableLogger(): void
  static create(config: LLamaConfig): LLama
  inference(params: LLamaArguments,
  callback: (result:
  { type: 'ERROR', message: string } |
  { type: 'DATA', data: InferenceToken } |
  { type: 'END' }
  ) => void): void
}
