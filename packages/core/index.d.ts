/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface InferenceToken {
  token: string
  completed: boolean
}
export const enum InferenceResultType {
  Data = 'Data',
  End = 'End',
  Error = 'Error'
}
export interface InferenceResult {
  type: InferenceResultType
  message?: string
  data?: InferenceToken
}
/**
 * Embedding result
*/
export const enum EmbeddingResultType {
  Data = 'Data',
  Error = 'Error'
}
export interface EmbeddingResult {
  type: EmbeddingResultType
  message?: string
  data?: Array<number>
}
/**
 * Tokenize result
*/
export const enum TokenizeResultType {
  Data = 'Data'
}
export interface TokenizeResult {
  type: TokenizeResultType
  data: Array<number>
}
/**
 * LLama model load config
*/
export interface LLamaConfig {
  path: string
  numCtxTokens?: number
  useMmap?: boolean
}
export interface LoadModelResult {
  error: boolean
  message?: string
}
export interface LLamaInferenceArguments {
  nThreads?: number
  nBatch?: number
  topK?: number
  topP?: number
  repeatPenalty?: number
  temp?: number
  seed?: number
  numPredict?: number
  repeatLastN?: number
  prompt: string
  float16?: boolean
  tokenBias?: string
  ignoreEos?: boolean
  feedPrompt?: boolean
}
export const enum ElementType {
  /** Quantized 4-bit (type 0). */
  Q4_0 = 0,
  /** Quantized 4-bit (type 1); used by GPTQ. */
  Q4_1 = 1,
  /** Float 16-bit. */
  F16 = 2,
  /** Float 32-bit. */
  F32 = 3
}
export function convert(path: string, elementType: ElementType): Promise<void>
export class LLama {
  static enableLogger(): void
  static create(config: LLamaConfig): LLama
  tokenize(params: string, callback: (result: TokenizeResult) => void): void
  getWordEmbeddings(params: LLamaInferenceArguments, callback: (result: EmbeddingResult) => void): void
  inference(params: LLamaInferenceArguments, callback: (result: InferenceResult) => void): void
}
