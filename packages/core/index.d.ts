/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface InferenceToken {
  token: string
  completed: boolean
}
export interface LoadModelResult {
  error: boolean
  message?: string
}
export interface TokenizeResult {
  data: Array<number>
}
export interface LLamaConfig {
  path: string
  numCtxTokens?: number
}
export interface LLamaInferenceArguments {
  nThreads?: number
  nBatch?: bigint
  topK?: bigint
  topP?: number
  repeatPenalty?: number
  temp?: number
  seed?: bigint
  numPredict?: bigint
  repeatLastN?: bigint
  prompt: string
  float16?: boolean
  tokenBias?: string
  ignoreEos?: boolean
  feedPrompt?: boolean
}
export class LLama {
  static enableLogger(): void
  static create(config: LLamaConfig): LLama
  tokenize(params: string,
  callback: (result:
  { type: 'DATA', data: number[] }
  ) => void): void
  getWordEmbeddings(params: LLamaInferenceArguments,
  callback: (result:
  { type: 'ERROR', message: string } |
  { type: 'DATA', data?: number[] }
  ) => void): void
  inference(params: LLamaInferenceArguments,
  callback: (result:
  { type: 'ERROR', message: string } |
  { type: 'DATA', data: InferenceToken } |
  { type: 'END' }
  ) => void): void
}
