[
  {
    "name": "tokenize",
    "markdown": "// tokenize.js\nimport { LLM } from \"llama-node\";\nimport { RwkvCpp } from \"llama-node/dist/llm/rwkv-cpp.js\";\nimport path from \"path\";\nconst modelPath = path.resolve(process.cwd(), \"../ggml-rwkv-4_raven-7b-v9-Eng99%-20230412-ctx8192-Q4_1_0.bin\");\nconst tokenizerPath = path.resolve(process.cwd(), \"../20B_tokenizer.json\");\nconst rwkv = new LLM(RwkvCpp);\nconst config = {\n    modelPath,\n    tokenizerPath,\n    nThreads: 4,\n    enableLogging: true,\n};\nconst run = async () => {\n    await rwkv.load(config);\n    await rwkv.tokenize({ content: \"hello world\" }).then(console.log);\n};\nrun();\n"
  },
  {
    "name": "inference",
    "markdown": "// inference.js\nimport { LLM } from \"llama-node\";\nimport { RwkvCpp } from \"llama-node/dist/llm/rwkv-cpp.js\";\nimport path from \"path\";\nconst modelPath = path.resolve(process.cwd(), \"../ggml-rwkv-4_raven-7b-v9-Eng99%-20230412-ctx8192-Q4_1_0.bin\");\nconst tokenizerPath = path.resolve(process.cwd(), \"../20B_tokenizer.json\");\nconst rwkv = new LLM(RwkvCpp);\nconst config = {\n    modelPath,\n    tokenizerPath,\n    nThreads: 4,\n    enableLogging: true,\n};\nconst template = `Who is the president of the United States?`;\nconst prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: ${template}\n\n### Response:`;\nconst params = {\n    maxPredictLength: 2048,\n    topP: 0.1,\n    temp: 0.1,\n    prompt,\n};\nconst run = async () => {\n    await rwkv.load(config);\n    await rwkv.createCompletion(params, (response) => {\n        process.stdout.write(response.token);\n    });\n};\nrun();\n"
  },
  {
    "name": "abortable",
    "markdown": "// abortable.js\nimport { LLM } from \"llama-node\";\nimport { RwkvCpp } from \"llama-node/dist/llm/rwkv-cpp.js\";\nimport path from \"path\";\nconst modelPath = path.resolve(process.cwd(), \"../ggml-rwkv-4_raven-7b-v9-Eng99%-20230412-ctx8192-Q4_1_0.bin\");\nconst tokenizerPath = path.resolve(process.cwd(), \"../20B_tokenizer.json\");\nconst rwkv = new LLM(RwkvCpp);\nconst config = {\n    modelPath,\n    tokenizerPath,\n    nThreads: 4,\n    enableLogging: true,\n};\nconst template = `Who is the president of the United States?`;\nconst prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction: ${template}\n\n### Response:`;\nconst params = {\n    maxPredictLength: 2048,\n    topP: 0.1,\n    temp: 0.1,\n    prompt,\n};\nconst run = async () => {\n    const abortController = new AbortController();\n    await rwkv.load(config);\n    setTimeout(() => {\n        abortController.abort();\n    }, 3000);\n    try {\n        await rwkv.createCompletion(params, (response) => {\n            process.stdout.write(response.token);\n        }, abortController.signal);\n    }\n    catch (e) {\n        console.log(e);\n    }\n};\nrun();\n"
  }
]