[
  {
    "name": "tokenize",
    "markdown": "// tokenize.js\nimport { LLM } from \"llama-node\";\nimport { LLMRS } from \"llama-node/dist/llm/llm-rs.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-alpaca-7b-q4.bin\");\nconst llama = new LLM(LLMRS);\nconst content = \"how are you?\";\nconst run = async () => {\n    await llama.load({ modelPath: model, modelType: \"Llama\" /* ModelType.Llama */ });\n    await llama.tokenize(content).then(console.log);\n};\nrun();\n"
  },
  {
    "name": "inference",
    "markdown": "// inference.js\nimport { LLM } from \"llama-node\";\nimport { LLMRS } from \"llama-node/dist/llm/llm-rs.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-alpaca-7b-q4.bin\");\nconst llama = new LLM(LLMRS);\nconst template = `how are you`;\nconst prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\n${template}\n\n### Response:`;\nconst params = {\n    prompt,\n    numPredict: 128,\n    temperature: 0.2,\n    topP: 1,\n    topK: 40,\n    repeatPenalty: 1,\n    repeatLastN: 64,\n    seed: 0,\n    feedPrompt: true,\n};\nconst run = async () => {\n    await llama.load({ modelPath: model, modelType: \"Llama\" /* ModelType.Llama */ });\n    await llama.createCompletion(params, (response) => {\n        process.stdout.write(response.token);\n    });\n};\nrun();\n"
  },
  {
    "name": "embedding",
    "markdown": "// embedding.js\nimport { LLM } from \"llama-node\";\nimport { LLMRS } from \"llama-node/dist/llm/llm-rs.js\";\nimport path from \"path\";\nimport fs from \"fs\";\nconst model = path.resolve(process.cwd(), \"../ggml-alpaca-7b-q4.bin\");\nconst llama = new LLM(LLMRS);\nconst getWordEmbeddings = async (prompt, file) => {\n    const data = await llama.getEmbedding({\n        prompt,\n        numPredict: 128,\n        temperature: 0.2,\n        topP: 1,\n        topK: 40,\n        repeatPenalty: 1,\n        repeatLastN: 64,\n        seed: 0,\n    });\n    console.log(prompt, data);\n    await fs.promises.writeFile(path.resolve(process.cwd(), file), JSON.stringify(data));\n};\nconst run = async () => {\n    await llama.load({ modelPath: model, modelType: \"Llama\" /* ModelType.Llama */ });\n    const dog1 = `My favourite animal is the dog`;\n    await getWordEmbeddings(dog1, \"./example/semantic-compare/dog1.json\");\n    const dog2 = `I have just adopted a cute dog`;\n    await getWordEmbeddings(dog2, \"./example/semantic-compare/dog2.json\");\n    const cat1 = `My favourite animal is the cat`;\n    await getWordEmbeddings(cat1, \"./example/semantic-compare/cat1.json\");\n};\nrun();\n"
  },
  {
    "name": "abortable",
    "markdown": "// abortable.js\nimport { LLM } from \"llama-node\";\nimport { LLMRS } from \"llama-node/dist/llm/llm-rs.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-alpaca-7b-q4.bin\");\nconst llama = new LLM(LLMRS);\nconst template = `how are you`;\nconst prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\n${template}\n\n### Response:`;\nconst params = {\n    prompt,\n    numPredict: 128,\n    temperature: 0.2,\n    topP: 1,\n    topK: 40,\n    repeatPenalty: 1,\n    repeatLastN: 64,\n    seed: 0,\n    feedPrompt: true,\n};\nconst run = async () => {\n    const abortController = new AbortController();\n    await llama.load({ modelPath: model, modelType: \"Llama\" /* ModelType.Llama */ });\n    setTimeout(() => {\n        abortController.abort();\n    }, 3000);\n    try {\n        await llama.createCompletion(params, (response) => {\n            process.stdout.write(response.token);\n        }, abortController.signal);\n    }\n    catch (e) {\n        console.log(e);\n    }\n};\nrun();\n"
  }
]