[
  {
    "name": "tokenize",
    "markdown": "// tokenize.js\nimport { LLM } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vic7b-q5_1.bin\");\nconst llama = new LLM(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: false,\n    useMmap: true,\n};\nconst content = \"how are you?\";\nconst run = async () => {\n    await llama.load(config);\n    await llama.tokenize({ content, nCtx: 2048 }).then(console.log);\n};\nrun();\n"
  },
  {
    "name": "inference",
    "markdown": "// inference.js\nimport { LLM } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vic7b-q5_1.bin\");\nconst llama = new LLM(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: false,\n    useMmap: true,\n};\nconst template = `How are you?`;\nconst prompt = `A chat between a user and an assistant.\nUSER: ${template}\nASSISTANT:`;\nconst params = {\n    nThreads: 4,\n    nTokPredict: 2048,\n    topK: 40,\n    topP: 0.1,\n    temp: 0.2,\n    repeatPenalty: 1,\n    prompt,\n};\nconst run = async () => {\n    await llama.load(config);\n    await llama.createCompletion(params, (response) => {\n        process.stdout.write(response.token);\n    });\n};\nrun();\n"
  },
  {
    "name": "embedding",
    "markdown": "// embedding.js\nimport { LLM } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vic7b-q5_1.bin\");\nconst llama = new LLM(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: true,\n    useMmap: true,\n};\nconst prompt = `Who is the president of the United States?`;\nconst params = {\n    nThreads: 4,\n    nTokPredict: 2048,\n    topK: 40,\n    topP: 0.1,\n    temp: 0.2,\n    repeatPenalty: 1,\n    prompt,\n};\nconst run = async () => {\n    await llama.load(config);\n    await llama.getEmbedding(params).then(console.log);\n};\nrun();\n"
  },
  {
    "name": "abortable",
    "markdown": "// abortable.js\nimport { LLM } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vic7b-q5_1.bin\");\nconst llama = new LLM(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: false,\n    useMmap: true,\n};\nconst template = `How are you?`;\nconst prompt = `A chat between a user and an assistant.\nUSER: ${template}\nASSISTANT:`;\nconst params = {\n    nThreads: 4,\n    nTokPredict: 2048,\n    topK: 40,\n    topP: 0.1,\n    temp: 0.2,\n    repeatPenalty: 1,\n    prompt,\n};\nconst run = async () => {\n    const abortController = new AbortController();\n    await llama.load(config);\n    setTimeout(() => {\n        abortController.abort();\n    }, 3000);\n    try {\n        await llama.createCompletion(params, (response) => {\n            process.stdout.write(response.token);\n        }, abortController.signal);\n    }\n    catch (e) {\n        console.log(e);\n    }\n};\nrun();\n"
  }
]