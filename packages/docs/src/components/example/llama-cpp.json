[
  {
    "name": "tokenize",
    "markdown": "// tokenize.js\nimport { LLama } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vicuna-7b-1.1-q4_1.bin\");\nconst llama = new LLama(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: false,\n    useMmap: true,\n};\nllama.load(config);\nconst content = \"how are you?\";\nllama.tokenize({ content, nCtx: 2048 }).then(console.log);\n"
  },
  {
    "name": "inference",
    "markdown": "// inference.js\nimport { LLama } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vicuna-7b-1.1-q4_1.bin\");\nconst llama = new LLama(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: false,\n    useMmap: true,\n};\nllama.load(config);\nconst template = `How are you?`;\nconst prompt = `### Human:\n\n${template}\n\n### Assistant:`;\nllama.createCompletion({\n    nThreads: 4,\n    nTokPredict: 2048,\n    topK: 40,\n    topP: 0.1,\n    temp: 0.2,\n    repeatPenalty: 1,\n    stopSequence: \"### Human\",\n    prompt,\n}, (response) => {\n    process.stdout.write(response.token);\n});\n"
  },
  {
    "name": "embedding",
    "markdown": "// embedding.js\nimport { LLama } from \"llama-node\";\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\nimport path from \"path\";\nconst model = path.resolve(process.cwd(), \"../ggml-vicuna-7b-1.1-q4_1.bin\");\nconst llama = new LLama(LLamaCpp);\nconst config = {\n    path: model,\n    enableLogging: true,\n    nCtx: 1024,\n    nParts: -1,\n    seed: 0,\n    f16Kv: false,\n    logitsAll: false,\n    vocabOnly: false,\n    useMlock: false,\n    embedding: true,\n    useMmap: true,\n};\nllama.load(config);\nconst prompt = `Who is the president of the United States?`;\nconst params = {\n    nThreads: 4,\n    nTokPredict: 2048,\n    topK: 40,\n    topP: 0.1,\n    temp: 0.2,\n    repeatPenalty: 1,\n    prompt,\n};\nllama.getEmbedding(params).then(console.log);\n"
  }
]