/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface LlamaInvocation {
  nThreads: number
  nTokPredict: number
  topK: number
  topP: number
  temp: number
  repeatPenalty: number
  stopSequence?: string
  prompt: string
}
export interface LlamaContextParams {
  nCtx: number
  nParts: number
  seed: number
  f16Kv: boolean
  logitsAll: boolean
  vocabOnly: boolean
  useMlock: boolean
  embedding: boolean
}
export const enum TokenizeResultType {
  Error = 0,
  Data = 1
}
export interface TokenizeResult {
  type: TokenizeResultType
  data: Array<number>
}
export interface InferenceToken {
  token: string
  completed: boolean
}
export const enum InferenceResultType {
  Error = 0,
  Data = 1,
  End = 2
}
export interface InferenceResult {
  type: InferenceResultType
  data?: InferenceToken
  message?: string
}
export const enum EmbeddingResultType {
  Error = 0,
  Data = 1
}
export interface EmbeddingResult {
  type: EmbeddingResultType
  data: Array<number>
}
export class LLama {
  static load(path: string, params: LlamaContextParams | undefined | null, enableLogger: boolean): LLama
  getWordEmbedding(input: LlamaInvocation,
  callback: (result: EmbeddingResult) => void): void
  tokenize(params: string,
  nCtx: number,
  callback: (result:
  { type: TokenizeResultType, data: number[] }
  ) => void): void
  inference(input: LlamaInvocation,
  callback: (result: InferenceResult) => void): void
}
