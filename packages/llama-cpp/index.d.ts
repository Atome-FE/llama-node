/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface LlamaInvocation {
  nThreads: number
  nTokPredict: number
  topK: number
  topP: number
  temp: number
  repeatPenalty: number
  stopSequence?: string
  prompt: string
}
export interface LlamaContextParams {
  nCtx: number
  nParts: number
  seed: number
  f16Kv: boolean
  logitsAll: boolean
  vocabOnly: boolean
  useMlock: boolean
  embedding: boolean
}
export interface InferenceToken {
  token: string
  completed: boolean
}
export const enum InferenceResultType {
  Error = 0,
  Data = 1,
  End = 2
}
export interface InferenceResult {
  type: InferenceResultType
  data?: InferenceToken
  message?: string
}
export class LLama {
  static load(path: string, params: LlamaContextParams | undefined | null, enableLogger: boolean): LLama
  inference(input: LlamaInvocation,
  callback: (result: InferenceResult) => void): void
}
