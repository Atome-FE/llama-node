/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface InferenceData {
  token: string
  completed: boolean
}
export interface LoadParams {
  path: string
  numCtxTokens?: number
}
export interface InferenceParams {
  nThreads?: number
  nBatch?: bigint
  topK?: bigint
  topP?: number
  repeatPenalty?: number
  temp?: number
  seed?: bigint
  numPredict?: bigint
  repeatLastN?: bigint
  prompt: string
}
export class LLama {
  static new(): LLama
  loadModel(params: LoadParams): void
  inference(params: InferenceParams): void
  onGenerated(callback: (err: null | Error, result: { token: string; completed: number }) => void): void
}
