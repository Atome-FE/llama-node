/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface InferenceToken {
  token: string
  completed: boolean
}
export interface LLamaConfig {
  path: string
  numCtxTokens?: number
}
export interface LLamaArguments {
  nThreads?: number
  nBatch?: bigint
  topK?: bigint
  topP?: number
  repeatPenalty?: number
  temp?: number
  seed?: bigint
  numPredict?: bigint
  repeatLastN?: bigint
  prompt: string
}
export class LLama {
  static create(config: LLamaConfig): LLama
  inference(params: LLamaArguments): void
  terminate(): void
  onGenerated(callback: (result: { type: 'ERROR', message: string } | { type: 'DATA', data: { token: string; completed: number } }) => void): void
}
